{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from scratch.linear_algebra import Vector, dot\n",
        "\n",
        "def sum_of_squares(v: Vector) -> float:\n",
        "    \"\"\"Computes the sum of squared elements in v\"\"\"\n",
        "    return dot(v, v)\n",
        "\n",
        "from typing import Callable\n",
        "\n",
        "def difference_quotient(f: Callable[[float], float],\n",
        "                        x: float,\n",
        "                        h: float) -> float:\n",
        "    return (f(x + h) - f(x)) / h\n",
        "\n",
        "def square(x: float) -> float:\n",
        "    return x * x\n",
        "\n",
        "def derivative(x: float) -> float:\n",
        "    return 2 * x\n",
        "\n",
        "def estimate_gradient(f: Callable[[Vector], float],\n",
        "                      v: Vector,\n",
        "                      h: float = 0.0001):\n",
        "    return [partial_difference_quotient(f, v, i, h)\n",
        "            for i in range(len(v))]\n",
        "\n",
        "import random\n",
        "from scratch.linear_algebra import distance, add, scalar_multiply\n",
        "\n",
        "def gradient_step(v: Vector, gradient: Vector, step_size: float) -> Vector:\n",
        "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
        "    assert len(v) == len(gradient)\n",
        "    step = scalar_multiply(step_size, gradient)\n",
        "    return add(v, step)\n",
        "\n",
        "def sum_of_squares_gradient(v: Vector) -> Vector:\n",
        "    return [2 * v_i for v_i in v]\n",
        "\n",
        "# x ranges from -50 to 49, y is always 20 * x + 5\n",
        "inputs = [(x, 20 * x + 5) for x in range(-50, 50)]\n",
        "\n",
        "def linear_gradient(x: float, y: float, theta: Vector) -> Vector:\n",
        "    slope, intercept = theta\n",
        "    predicted = slope * x + intercept    # The prediction of the model.\n",
        "    error = (predicted - y)              # error is (predicted - actual)\n",
        "    squared_error = error ** 2           # We'll minimize squared error\n",
        "    grad = [2 * error * x, 2 * error]    # using its gradient.\n",
        "    return grad\n",
        "\n",
        "from typing import TypeVar, List, Iterator\n",
        "\n",
        "T = TypeVar('T')  # this allows us to type \"generic\" functions\n",
        "\n",
        "def minibatches(dataset: List[T],\n",
        "                batch_size: int,\n",
        "                shuffle: bool = True) -> Iterator[List[T]]:\n",
        "    \"\"\"Generates `batch_size`-sized minibatches from the dataset\"\"\"\n",
        "    # Start indexes 0, batch_size, 2 * batch_size, ...\n",
        "    batch_starts = [start for start in range(0, len(dataset), batch_size)]\n",
        "\n",
        "    if shuffle: random.shuffle(batch_starts)  # shuffle the batches\n",
        "\n",
        "    for start in batch_starts:\n",
        "        end = start + batch_size\n",
        "        yield dataset[start:end]\n",
        "\n",
        "def main():\n",
        "    xs = range(-10, 11)\n",
        "    actuals = [derivative(x) for x in xs]\n",
        "    estimates = [difference_quotient(square, x, h=0.001) for x in xs]\n",
        "    \n",
        "    # plot to show they're basically the same\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.title(\"Actual Derivatives vs. Estimates\")\n",
        "    plt.plot(xs, actuals, 'rx', label='Actual')       # red  x\n",
        "    plt.plot(xs, estimates, 'b+', label='Estimate')   # blue +\n",
        "    plt.legend(loc=9)\n",
        "    # plt.show()\n",
        "    \n",
        "    \n",
        "    plt.close()\n",
        "    \n",
        "    def partial_difference_quotient(f: Callable[[Vector], float],\n",
        "                                    v: Vector,\n",
        "                                    i: int,\n",
        "                                    h: float) -> float:\n",
        "        \"\"\"Returns the i-th partial difference quotient of f at v\"\"\"\n",
        "        w = [v_j + (h if j == i else 0)    # add h to just the ith element of v\n",
        "             for j, v_j in enumerate(v)]\n",
        "    \n",
        "        return (f(w) - f(v)) / h\n",
        "    \n",
        "    \n",
        "    # \"Using the Gradient\" example\n",
        "    \n",
        "    # pick a random starting point\n",
        "    v = [random.uniform(-10, 10) for i in range(3)]\n",
        "    \n",
        "    for epoch in range(1000):\n",
        "        grad = sum_of_squares_gradient(v)    # compute the gradient at v\n",
        "        v = gradient_step(v, grad, -0.01)    # take a negative gradient step\n",
        "        print(epoch, v)\n",
        "    \n",
        "    assert distance(v, [0, 0, 0]) < 0.001    # v should be close to 0\n",
        "    \n",
        "    \n",
        "    # First \"Using Gradient Descent to Fit Models\" example\n",
        "    \n",
        "    from scratch.linear_algebra import vector_mean\n",
        "    \n",
        "    # Start with random values for slope and intercept.\n",
        "    theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
        "    \n",
        "    learning_rate = 0.001\n",
        "    \n",
        "    for epoch in range(5000):\n",
        "        # Compute the mean of the gradients\n",
        "        grad = vector_mean([linear_gradient(x, y, theta) for x, y in inputs])\n",
        "        # Take a step in that direction\n",
        "        theta = gradient_step(theta, grad, -learning_rate)\n",
        "        print(epoch, theta)\n",
        "    \n",
        "    slope, intercept = theta\n",
        "    assert 19.9 < slope < 20.1,   \"slope should be about 20\"\n",
        "    assert 4.9 < intercept < 5.1, \"intercept should be about 5\"\n",
        "    \n",
        "    \n",
        "    # Minibatch gradient descent example\n",
        "    \n",
        "    theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
        "    \n",
        "    for epoch in range(1000):\n",
        "        for batch in minibatches(inputs, batch_size=20):\n",
        "            grad = vector_mean([linear_gradient(x, y, theta) for x, y in batch])\n",
        "            theta = gradient_step(theta, grad, -learning_rate)\n",
        "        print(epoch, theta)\n",
        "    \n",
        "    slope, intercept = theta\n",
        "    assert 19.9 < slope < 20.1,   \"slope should be about 20\"\n",
        "    assert 4.9 < intercept < 5.1, \"intercept should be about 5\"\n",
        "    \n",
        "    \n",
        "    # Stochastic gradient descent example\n",
        "    \n",
        "    theta = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
        "    \n",
        "    for epoch in range(100):\n",
        "        for x, y in inputs:\n",
        "            grad = linear_gradient(x, y, theta)\n",
        "            theta = gradient_step(theta, grad, -learning_rate)\n",
        "        print(epoch, theta)\n",
        "    \n",
        "    slope, intercept = theta\n",
        "    assert 19.9 < slope < 20.1,   \"slope should be about 20\"\n",
        "    assert 4.9 < intercept < 5.1, \"intercept should be about 5\"\n",
        "    \n",
        "if __name__ == \"__main__\": main()"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}